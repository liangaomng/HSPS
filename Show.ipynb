{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "def VMD_for_nD_signal_torch(f,alpha,tau,K,tol,Niter):\n",
    "    ltemp = f.shape[1]//2 \n",
    "    fs=1./f.shape[1]\n",
    "    fMirr =  torch.cat((torch.flip(f[:,:ltemp],dims = [1]),f),1)\n",
    "    fMirr =  torch.cat((fMirr,torch.flip(f[:,-ltemp:],dims = [1])),1)\n",
    "    T=fMirr.shape[1]\n",
    "    t1 = torch.arange(1,T+1)/T  \n",
    "    t=torch.tile(t1,(f.shape[0],1))\n",
    "    freqs = t-0.5-(1/T)\n",
    "    freqs=freqs.to(\"cpu\")\n",
    "    f_hat = torch.fft.fftshift((torch.fft.fft(fMirr)),dim=[1])\n",
    "    f_hat_plus = torch.clone (f_hat)\n",
    "    f_hat_plus[:,:T//2] = 0\n",
    "    omega_plus = torch.zeros([f.shape[0], K])\n",
    "    lambda_hat_real = torch.zeros([f.shape[0], T])\n",
    "    lambda_hat_imag = torch.zeros([f.shape[0], T])\n",
    "    lambda_hat=torch.complex(lambda_hat_real,lambda_hat_imag)\n",
    "    uDiff = tol+np.spacing(1)\n",
    "    uDiff_max=uDiff\n",
    "    n = 0 \n",
    "    sum_uk = 0#\n",
    "    u_hat_plus_real = torch.zeros([f.shape[0], T, K]) \n",
    "    u_hat_plus_imag = torch.zeros([f.shape[0], T, K])\n",
    "    u_hat_plus=torch.complex(u_hat_plus_real,u_hat_plus_imag)\n",
    "    u_hat_plus_cur=torch.clone(u_hat_plus)\n",
    "    lambda_hat_cur=torch.clone(lambda_hat)\n",
    "    omega_plus_cur=torch.clone(omega_plus)\n",
    "    while ( uDiff_max > tol and  n < Niter-1 ):\n",
    "        k = 0\n",
    "        sum_uk = u_hat_plus[:,:,K-1] + sum_uk - u_hat_plus[:,:,0]\n",
    "        a=(f_hat_plus - sum_uk - lambda_hat[:]/2)\n",
    "        b=(1.+alpha*(freqs - torch.tile(omega_plus[:,k].reshape(f.shape[0],1),(1,T)))**2)\n",
    "        u_hat_plus_cur[:,:,k]= a/b\n",
    "        c1=abs(u_hat_plus_cur[:,T//2:T,k])**2\n",
    "        d1=freqs[:,T//2:T]\n",
    "        e1=torch.diagonal(torch.matmul(c1,d1.t()))\n",
    "        f1=torch.sum(abs(u_hat_plus_cur[:,T//2:T,k])**2,1)\n",
    "        omega_plus_cur[:,k] = e1/f1\n",
    "        for k in np.arange(1,K):\n",
    "            sum_uk = u_hat_plus_cur[:,:,k-1] + sum_uk - u_hat_plus[:,:,k]\n",
    "            a=(f_hat_plus - sum_uk - lambda_hat[:]/2)\n",
    "            b=(1.+alpha*(freqs - torch.tile(omega_plus[:,k].reshape(f.shape[0],1),(1,T)))**2)\n",
    "            u_hat_plus_cur[:,:,k]= a/b\n",
    "            c2=abs(u_hat_plus_cur[:,T//2:T,k])**2\n",
    "            d2=freqs[:,T//2:T]\n",
    "            e2=torch.diagonal(torch.matmul(c2,d2.t()))\n",
    "            f2=torch.sum(abs(u_hat_plus_cur[:,T//2:T,k])**2,1)\n",
    "            omega_plus_cur[:,k] = e2/f2\n",
    "        lambda_hat_cur[:,:] = lambda_hat[:,:] + tau*(torch.sum(u_hat_plus_cur[:,:,:],dim=[2]) - f_hat_plus)\n",
    "        uDiff = np.spacing(1)\n",
    "        for i in range(K):\n",
    "            uDiff = uDiff + (1/T)*torch.matmul((u_hat_plus_cur[:,:,i]-u_hat_plus[:,:,i]),(torch.conj((u_hat_plus_cur[:,:,i]-u_hat_plus[:,:,i])).t()))\n",
    "        uDiff =torch.abs(uDiff)\n",
    "        uDiff_max=min(torch.diagonal(uDiff))\n",
    "        u_hat_plus_pre=torch.clone(u_hat_plus)\n",
    "        lambda_hat_pre=torch.clone(lambda_hat)\n",
    "        omega_plus_pre=torch.clone(omega_plus)\n",
    "        u_hat_plus=torch.clone(u_hat_plus_cur)\n",
    "        lambda_hat=torch.clone(lambda_hat_cur)\n",
    "        omega_plus=torch.clone(omega_plus_cur)\n",
    "        n=n+1\n",
    "    omega = omega_plus_pre\n",
    "    idxs = torch.flip(torch.arange(1,T//2+1),dims = [0])\n",
    "    idxs=idxs.to(\"cpu\")\n",
    "    u_hat_real = torch.zeros([f.shape[0], T, K])\n",
    "    u_hat_imag = torch.zeros([f.shape[0], T, K])\n",
    "    u_hat=torch.complex(u_hat_real,u_hat_imag)\n",
    "\n",
    "    u_hat[:,T//2:T,:] = u_hat_plus_pre[:,T//2:T,:]\n",
    "    u_hat[:,idxs,:] = torch.conj(u_hat_plus_pre[:,T//2:T,:])\n",
    "    u_hat[:,0,:] = torch.conj(u_hat_plus_pre[:,-1,:])   \n",
    "    u = torch.zeros([f.shape[0],K,T]).cuda()\n",
    "    for k in range(K):\n",
    "        u[:,k,:] = torch.real(torch.fft.ifft(torch.fft.ifftshift(u_hat[:,:,k],dim=1)))\n",
    "    u = u[:,:,T//4:3*T//4]\n",
    "\n",
    "    u_hat_real = torch.zeros([f.shape[0],u.shape[2], K]).cuda()\n",
    "    u_hat_imag = torch.zeros([f.shape[0],u.shape[2], K]).cuda()\n",
    "    u_hat=torch.complex(u_hat_real,u_hat_imag)\n",
    "\n",
    "    for k in range(K):\n",
    "        u_hat[:,:,k] = torch.fft.fftshift(torch.fft.fft(u[:,k,:]),dim=1)\n",
    "    return u, u_hat, omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m Niter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 调用VMD函数\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m u, u_hat, omega \u001b[38;5;241m=\u001b[39m \u001b[43mVMD_for_nD_signal_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 查看结果\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(u\u001b[38;5;241m.\u001b[39mshape, u_hat\u001b[38;5;241m.\u001b[39mshape, omega\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[28], line 17\u001b[0m, in \u001b[0;36mVMD_for_nD_signal_torch\u001b[0;34m(f, alpha, tau, K, tol, Niter)\u001b[0m\n\u001b[1;32m     15\u001b[0m f_hat_plus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone (f_hat)\n\u001b[1;32m     16\u001b[0m f_hat_plus[:,:T\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m omega_plus \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m lambda_hat_real \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], T])\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     19\u001b[0m lambda_hat_imag \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], T])\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例信号\n",
    "signal = np.random.randn(1, 1000)\n",
    "signal = torch.tensor(signal, dtype=torch.float32)\n",
    "\n",
    "# 设置VMD参数\n",
    "alpha = 2000\n",
    "tau = 0\n",
    "K = 3\n",
    "tol = 1e-7\n",
    "Niter = 1000\n",
    "\n",
    "# 调用VMD函数\n",
    "u, u_hat, omega = VMD_for_nD_signal_torch(signal, alpha, tau, K, tol, Niter)\n",
    "\n",
    "# 查看结果\n",
    "print(u.shape, u_hat.shape, omega.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
